---
title: "ConViT: Improving Vision Transformers with Soft Convolutional Inductive Biases"
collection: publications
date: 2021-03-21
venue: arXiv
paperurl: "https://arxiv.org/abs/2103.10697"
citation: S d'Ascoli, H Touvron, <b>ML Leavitt</b>, AS Morcos, G Biroli, L Sagun
image: /images/publications-2021-03-convit.png
---
Under review in an archival venue

<i>Accessible abstract:</i> The majority of progress in computer vision in the last decade has come from the use and refinement of convolutional neural networks (CNNs). The convolutional architecture enables sample-efficiency, but comes at the cost of a potentially lower performance ceiling. Transformers, a model architecture traditionally used for language tasks, have recently shown promise for vision tasks. Transformers are distinguished by their "self-attention" layers, which allow them to learn relationships between distant parts of their inputs, for example between the beginning and end of a sentence, or two opposite corners of an image. However these vision transformers (ViTs) require costly pre-training on large external datasets or teaching signals (for the experts: distillation) from pre-trained CNNs. In this paper, we sought to combine the strengths of these two architectures while avoiding their respective limitations. We accomplished this by adding a component to the self-attention layers of ViTs that performs convolution, but critically, the network isn't stuck with convolution; it learns the optimal trade-off between convolution and traditional self-attention. We call this convolution-like ViT architecture ConViT. ConViT outperforms existing ViT models on multiple image classification benchmarks, and particularly excels when data are limited and/or models are large. We also examine the learning dynamics and inner workings of ConViT and other ViT models to better understand how they perform image classification.  