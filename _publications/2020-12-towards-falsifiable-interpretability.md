---
title: "Towards falsifiable interpretability research"
collection: publications
date: 2020-12-11
venue: 'ML Retrospectives, Surveys & Meta-Analyses @NeurIPS'
paperurl: "https://ml-retrospectives.github.io/neurips2020/camera_ready/4.pdf"
citation: <b>ML Leavitt</b>, AS Morcos
image:
---
<i>Accessible abstract:</i> Methods for understanding and interpreting deep neural networks (DNNs) typically rely on building intuition by using visualizations or focusing on the properties of individual data samples (e.g. images) or neurons. In this paper, we argue that DNN interpretability research suffers from an over-reliance on intuition that risks—and in some cases has caused—illusory progress and misleading conclusions. We identify a set of impediments to meaningful progress in interpretability research, and examine two case studies that highlight the ill effects of these impediments. We then propose a strategy to address these impediments in the form of a framework for strong, falsifiable interpretability research. Intuition is an indispensable first step, but it has just as much power to mislead as it does to enlighten. To fully understand AI systems, we must strive for methods that are not just intuitive but also empirically grounded.
